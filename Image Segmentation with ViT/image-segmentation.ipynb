{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ§  Human Parsing with Semantic Segmentation\n\nThis notebook demonstrates training a semantic segmentation model to perform **human parsing** using the `mattmdjaga/human_parsing_dataset` (based on the ATR dataset). The goal is to segment fine-grained human parts (e.g., hat, hair, clothes, limbs) at the pixel level using a transformer-based architecture like **Segformer** from Hugging Face.\n\n## ðŸ“¦ Dataset\n- Source: `mattmdjaga/human_parsing_dataset` (17,706 images with mask annotations)\n- Classes: 18 including `\"Hat\"`, `\"Hair\"`, `\"Sunglasses\"`, `\"Upper-clothes\"`, `\"Left-arm\"`, `\"Right-leg\"`, `\"Bag\"`, etc.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### ***There are several types of segmentation: semantic segmentation, instance segmentation, and panoptic segmentation.*** <br> In this notebook I will cover semantic segmentation","metadata":{}},{"cell_type":"code","source":"# install the necessary libraries\n!pip install -q datasets transformers evaluate accelerate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# setting huggingface env\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Semantic Segmentation\n#### Choose ***human_parsing_dataset*** dataset","metadata":{}},{"cell_type":"code","source":"\nfrom datasets import load_dataset\n\nds = load_dataset(\"mattmdjaga/human_parsing_dataset\", split=\"train[:1000]\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = ds.train_test_split(test_size=0.2)\ntrain_ds = ds[\"train\"]\ntest_ds = ds[\"test\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds[0]\n\n# train_ds[0][\"image\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"create a dictionary that maps a label id to a label class which will be useful when you set up the model later.","metadata":{}},{"cell_type":"code","source":"id2label = {\n    0: \"Background\",\n    1: \"Hat\",\n    2: \"Hair\",\n    3: \"Sunglasses\",\n    4: \"Upper-clothes\",\n    5: \"Skirt\",\n    6: \"Pants\",\n    7: \"Dress\",\n    8: \"Belt\",\n    9: \"Left-shoe\",\n    10: \"Right-shoe\",\n    11: \"Face\",\n    12: \"Left-leg\",\n    13: \"Right-leg\",\n    14: \"Left-arm\",\n    15: \"Right-arm\",\n    16: \"Bag\",\n    17: \"Scarf\"\n}\n\nlabel2id = {v: k for k, v in id2label.items()}\nnum_labels = len(id2label)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # If we have custom dataset, we can use this portion for the code\n# from datasets import Dataset, DatasetDict, Image\n\n# image_paths_train = [\"path/to/image_1.jpg/jpg\", \"path/to/image_2.jpg/jpg\", ..., \"path/to/image_n.jpg/jpg\"]\n# label_paths_train = [\"path/to/annotation_1.png\", \"path/to/annotation_2.png\", ..., \"path/to/annotation_n.png\"]\n\n# image_paths_validation = [...]\n# label_paths_validation = [...]\n\n# def create_dataset(image_paths, label_paths):\n#     dataset = Dataset.from_dict({\"image\": sorted(image_paths),\n#                                 \"label\": sorted(label_paths)})\n#     dataset = dataset.cast_column(\"image\", Image())\n#     dataset = dataset.cast_column(\"label\", Image())\n#     return dataset\n\n# # step 1: create Dataset objects\n# train_dataset = create_dataset(image_paths_train, label_paths_train)\n# validation_dataset = create_dataset(image_paths_validation, label_paths_validation)\n\n# # step 2: create DatasetDict\n# dataset = DatasetDict({\n#      \"train\": train_dataset,\n#      \"validation\": validation_dataset,\n#      }\n# )\n\n# # step 3: push to Hub (assumes you have ran the huggingface-cli login command in a terminal/notebook)\n# dataset.push_to_hub(\"your-name/dataset-repo\")\n\n# # optionally, you can push to a private repo on the Hub\n# # dataset.push_to_hub(\"name of repo on the hub\", private=True)\n\n# import json\n# # simple example\n# id2label = {0: 'cat', 1: 'dog'}\n# with open('id2label.json', 'w') as fp:\n# json.dump(id2label, fp)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# setting image processor\nfrom transformers import AutoImageProcessor\n# do_reduce_labels=True to subtract one from all the labels.\nmodel_name = \"nvidia/mit-b0\"\nimage_processor = AutoImageProcessor.from_pretrained(model_name, do_reduce_labels=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Data Augmentations with ColorJitter to make a model more robust against overfitting. ","metadata":{}},{"cell_type":"code","source":"from torchvision.transforms import ColorJitter\n\njitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_transforms(example_batch):\n    images = [jitter(x) for x in example_batch[\"image\"]]\n    labels = [x for x in example_batch[\"mask\"]]\n    inputs = image_processor(images, labels)\n    return inputs\n\n\ndef val_transforms(example_batch):\n    images = [x for x in example_batch[\"image\"]]\n    labels = [x for x in example_batch[\"mask\"]]\n    inputs = image_processor(images, labels)\n    return inputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Applying transform\ntrain_ds.set_transform(train_transforms)\ntest_ds.set_transform(val_transforms)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds, test_ds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Set Evaluate metric","metadata":{}},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"mean_iou\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"predictions need to be converted to logits first, and then reshaped to match the size of the labels before calling compute:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn\n\ndef compute_metrics(eval_pred):\n    with torch.no_grad():\n        logits, labels = eval_pred\n        logits_tensor = torch.from_numpy(logits)\n        logits_tensor = nn.functional.interpolate(\n            logits_tensor,\n            size=labels.shape[-2:],\n            mode=\"bilinear\",\n            align_corners=False,\n        ).argmax(dim=1)\n\n        pred_labels = logits_tensor.detach().cpu().numpy()\n        metrics = metric.compute(\n            predictions=pred_labels,\n            references=labels,\n            num_labels=num_labels,\n            ignore_index=255,\n            reduce_labels=False,\n        )\n        for key, value in metrics.items():\n            if isinstance(value, np.ndarray):\n                metrics[key] = value.tolist()\n        return metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Now Training","metadata":{}},{"cell_type":"code","source":"# Defining Model, i did only for SemanticSegmentation\nfrom transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n\nmodel = AutoModelForSemanticSegmentation.from_pretrained(model_name, id2label=id2label, label2id=label2id)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setting Training Arguments\nfrom transformers import TrainingArguments\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"my-human_parsing-model\",\n    run_name=\"segment-human_parsing-1\",\n    learning_rate=6e-5,\n    num_train_epochs=5,\n    per_device_train_batch_size=5,\n    per_device_eval_batch_size=5,\n    save_total_limit=2,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    save_steps=20,\n    eval_steps=20,\n    logging_steps=1,  # <- Log after every step\n    logging_dir=\"./logs\",  # <- Optional but useful\n    remove_unused_columns=False,\n    push_to_hub=False,  # <- Disable for now\n    report_to=\"none\",  # <- Disable Weights & Biases, TensorBoard etc.\n    fp16=True  # <- Enable mixed precision if using GPU\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    compute_metrics=compute_metrics,\n)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Start the training\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"mattmdjaga/human_parsing_dataset\", split=\"train[:50]\")\nds = ds.train_test_split(test_size=0.2)\ntest_ds = ds[\"test\"]\nimage = ds[\"test\"][0][\"image\"]\nimage","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from accelerate.test_utils.testing import get_backend\ndevice, _, _ = get_backend()\nencoding = image_processor(image, return_tensors=\"pt\")\npixel_values = encoding.pixel_values.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Pass your input to the model and return the logits:\noutputs = model(pixel_values=pixel_values)\nlogits = outputs.logits.cpu()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# rescale the logits to the original image size:\nupsampled_logits = nn.functional.interpolate(\n    logits,\n    size=image.size[::-1],\n    mode=\"bilinear\",\n    align_corners=False,\n)\n\npred_seg = upsampled_logits.argmax(dim=1)[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To visualize the results, load the dataset color palette as ade_palette() that maps each class to their RGB values.","metadata":{}},{"cell_type":"code","source":"def ade_palette():\n  return np.asarray([\n      [0, 0, 0],\n      [120, 120, 120],\n      [180, 120, 120],\n      [6, 230, 230],\n      [80, 50, 50],\n      [4, 200, 3],\n      [120, 120, 80],\n      [140, 140, 140],\n      [204, 5, 255],\n      [230, 230, 230],\n      [4, 250, 7],\n      [224, 5, 255],\n      [235, 255, 7],\n      [150, 5, 61],\n      [120, 120, 70],\n      [8, 255, 51],\n      [255, 6, 82],\n      [143, 255, 140],\n      [204, 255, 4],\n      [255, 51, 7],\n      [204, 70, 3],\n      [0, 102, 200],\n      [61, 230, 250],\n      [255, 6, 51],\n      [11, 102, 255],\n      [255, 7, 71],\n      [255, 9, 224],\n      [9, 7, 230],\n      [220, 220, 220],\n      [255, 9, 92],\n      [112, 9, 255],\n      [8, 255, 214],\n      [7, 255, 224],\n      [255, 184, 6],\n      [10, 255, 71],\n      [255, 41, 10],\n      [7, 255, 255],\n      [224, 255, 8],\n      [102, 8, 255],\n      [255, 61, 6],\n      [255, 194, 7],\n      [255, 122, 8],\n      [0, 255, 20],\n      [255, 8, 41],\n      [255, 5, 153],\n      [6, 51, 255],\n      [235, 12, 255],\n      [160, 150, 20],\n      [0, 163, 255],\n      [140, 140, 140],\n      [250, 10, 15],\n      [20, 255, 0],\n      [31, 255, 0],\n      [255, 31, 0],\n      [255, 224, 0],\n      [153, 255, 0],\n      [0, 0, 255],\n      [255, 71, 0],\n      [0, 235, 255],\n      [0, 173, 255],\n      [31, 0, 255],\n      [11, 200, 200],\n      [255, 82, 0],\n      [0, 255, 245],\n      [0, 61, 255],\n      [0, 255, 112],\n      [0, 255, 133],\n      [255, 0, 0],\n      [255, 163, 0],\n      [255, 102, 0],\n      [194, 255, 0],\n      [0, 143, 255],\n      [51, 255, 0],\n      [0, 82, 255],\n      [0, 255, 41],\n      [0, 255, 173],\n      [10, 0, 255],\n      [173, 255, 0],\n      [0, 255, 153],\n      [255, 92, 0],\n      [255, 0, 255],\n      [255, 0, 245],\n      [255, 0, 102],\n      [255, 173, 0],\n      [255, 0, 20],\n      [255, 184, 184],\n      [0, 31, 255],\n      [0, 255, 61],\n      [0, 71, 255],\n      [255, 0, 204],\n      [0, 255, 194],\n      [0, 255, 82],\n      [0, 10, 255],\n      [0, 112, 255],\n      [51, 0, 255],\n      [0, 194, 255],\n      [0, 122, 255],\n      [0, 255, 163],\n      [255, 153, 0],\n      [0, 255, 10],\n      [255, 112, 0],\n      [143, 255, 0],\n      [82, 0, 255],\n      [163, 255, 0],\n      [255, 235, 0],\n      [8, 184, 170],\n      [133, 0, 255],\n      [0, 255, 92],\n      [184, 0, 255],\n      [255, 0, 31],\n      [0, 184, 255],\n      [0, 214, 255],\n      [255, 0, 112],\n      [92, 255, 0],\n      [0, 224, 255],\n      [112, 224, 255],\n      [70, 184, 160],\n      [163, 0, 255],\n      [153, 0, 255],\n      [71, 255, 0],\n      [255, 0, 163],\n      [255, 204, 0],\n      [255, 0, 143],\n      [0, 255, 235],\n      [133, 255, 0],\n      [255, 0, 235],\n      [245, 0, 255],\n      [255, 0, 122],\n      [255, 245, 0],\n      [10, 190, 212],\n      [214, 255, 0],\n      [0, 204, 255],\n      [20, 0, 255],\n      [255, 255, 0],\n      [0, 153, 255],\n      [0, 41, 255],\n      [0, 255, 204],\n      [41, 0, 255],\n      [41, 255, 0],\n      [173, 0, 255],\n      [0, 245, 255],\n      [71, 0, 255],\n      [122, 0, 255],\n      [0, 255, 184],\n      [0, 92, 255],\n      [184, 255, 0],\n      [0, 133, 255],\n      [255, 214, 0],\n      [25, 194, 194],\n      [102, 255, 0],\n      [92, 0, 255],\n  ])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ncolor_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)\npalette = np.array(ade_palette())\nfor label, color in enumerate(palette):\n    color_seg[pred_seg == label, :] = color\ncolor_seg = color_seg[..., ::-1]  # convert to BGR\n\nimg = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\nimg = img.astype(np.uint8)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(img)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}