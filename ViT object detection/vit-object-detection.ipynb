{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 🔐 Hugging Face Authentication\n\nThis notebook uses the Hugging Face Hub to load and fine-tune a pretrained object detection model.  \nAuthentication is required to access certain datasets or push models back to the Hub.\n\nWe authenticate securely using a Hugging Face token via environment variables or Colab secrets.\n\n### 🔐 Security Tip\n- **Never share your token in public notebooks or repos.**\n- Use `.env` files or Colab secrets to keep credentials safe.\n- Tokens can be managed at: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n\n---\n\n## 📦 Dataset\n- **Name:** `Francesco/animals-ij5d2`\n- **Type:** Object Detection (bounding boxes and labels)\n- **Format:** COCO-style annotation\n\n## 🧠 Model\n- **Model Name:** `facebook/detr-resnet-50`\n- **Type:** DETR (DEtection TRansformer)\n- **Task:** Fine-tuned for object detection on a custom 12-class animal dataset\n","metadata":{}},{"cell_type":"code","source":"!pip install -q datasets transformers accelerate timm\n!pip install -q -U albumentations>=1.4.5 torchmetrics pycocotools","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"facebook/detr-resnet-50\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading Sataset\nfrom datasets import load_dataset\n\nds = load_dataset(\"Francesco/animals-ij5d2\")\n\nif \"validation\" not in ds:\n    split = ds[\"train\"].train_test_split(0.20, seed=42)\n    ds[\"train\"] = split[\"train\"]\n    ds[\"validation\"] = split[\"test\"]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#visualize an example in the dataset.\nimport numpy as np\nimport os\nfrom PIL import Image, ImageDraw\n\nimage = ds[\"train\"][2][\"image\"]\nannotations = ds[\"train\"][2][\"objects\"]\ndraw = ImageDraw.Draw(image)\n\ncategories = ds[\"train\"].features[\"objects\"].feature[\"category\"].names\n\nid2label = {index: x for index, x in enumerate(categories, start=0)}\nlabel2id = {v: k for k, v in id2label.items()}\n\nfor i in range(len(annotations[\"id\"])):\n    box = annotations[\"bbox\"][i]\n    class_idx = annotations[\"category\"][i]\n    x, y, w, h = tuple(box)\n    # Check if coordinates are normalized or not\n    if max(box) > 1.0:\n        # Coordinates are un-normalized, no need to re-scale them\n        x1, y1 = int(x), int(y)\n        x2, y2 = int(x + w), int(y + h)\n    else:\n        # Coordinates are normalized, re-scale them\n        x1 = int(x * width)\n        y1 = int(y * height)\n        x2 = int((x + w) * width)\n        y2 = int((y + h) * height)\n    draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n    draw.text((x, y), id2label[class_idx], fill=\"white\")\n\nimage","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocess the dataset\nAutoImageProcessor takes care of processing image data to create pixel_values, pixel_mask, and labels that a DETR model can train with. ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoImageProcessor\n\nimage_size = 480\n\nimage_processor = AutoImageProcessor.from_pretrained(\n    model_name,\n    do_resize=True,\n    size={\"max_height\": image_size, \"max_width\": image_size},\n    do_pad=True,\n    pad_size={\"height\": image_size, \"width\": image_size},\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Agumentation and applying a certain format\nFor data augmentation i am using a library called Albumentations","metadata":{}},{"cell_type":"code","source":"import albumentations as A\n\ntrain_augment_and_transform = A.Compose(\n    [\n        A.Perspective(p=0.1),\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.HueSaturationValue(p=0.1),\n    ],\n    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True, min_area=25),\n)\n\nvalidation_transform = A.Compose(\n    [A.NoOp()],\n    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True),\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fixing the formay because mage_processor expects the annotations to be in the following format: {'image_id': int, 'annotations': List[Dict]}\ndef format_image_annotations_as_coco(image_id, categories, areas, bboxes):\n    \"\"\"Format one set of image annotations to the COCO format\n\n    Args:\n        image_id (str): image id. e.g. \"0001\"\n        categories (List[int]): list of categories/class labels corresponding to provided bounding boxes\n        areas (List[float]): list of corresponding areas to provided bounding boxes\n        bboxes (List[Tuple[float]]): list of bounding boxes provided in COCO format\n            ([center_x, center_y, width, height] in absolute coordinates)\n\n    Returns:\n        dict: {\n            \"image_id\": image id,\n            \"annotations\": list of formatted annotations\n        }\n    \"\"\"\n    annotations = []\n    for category, area, bbox in zip(categories, areas, bboxes):\n        formatted_annotation = {\n            \"image_id\": image_id,\n            \"category_id\": category,\n            \"iscrowd\": 0,\n            \"area\": area,\n            \"bbox\": list(bbox),\n        }\n        annotations.append(formatted_annotation)\n\n    return {\n        \"image_id\": image_id,\n        \"annotations\": annotations,\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now combine the image and annotation transformations to use on a batch of examples:\ndef augment_and_transform_batch(examples, transform, image_processor, return_pixel_mask=False):\n    \"\"\"Apply augmentations and format annotations in COCO format for object detection task\"\"\"\n\n    images = []\n    annotations = []\n    for image_id, image, objects in zip(examples[\"image_id\"], examples[\"image\"], examples[\"objects\"]):\n        image = np.array(image.convert(\"RGB\"))\n\n        # apply augmentations\n        output = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n        images.append(output[\"image\"])\n\n        # format annotations in COCO format\n        formatted_annotations = format_image_annotations_as_coco(\n            image_id, output[\"category\"], objects[\"area\"], output[\"bboxes\"]\n        )\n        annotations.append(formatted_annotations)\n\n    # Apply the image processor transformations: resizing, rescaling, normalization\n    result = image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n\n    if not return_pixel_mask:\n        result.pop(\"pixel_mask\", None)\n\n    return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Now applying those preprocess function with the dataset\nfrom functools import partial\nimport numpy as np\n\n# Make transform functions for batch and apply for dataset splits\ntrain_transform_batch = partial(\n    augment_and_transform_batch, transform=train_augment_and_transform, image_processor=image_processor\n)\nvalidation_transform_batch = partial(\n    augment_and_transform_batch, transform=validation_transform, image_processor=image_processor\n)\n\nds[\"train\"] = ds[\"train\"].with_transform(train_transform_batch)\nds[\"validation\"] = ds[\"validation\"].with_transform(validation_transform_batch)\nds[\"test\"] = ds[\"test\"].with_transform(validation_transform_batch)\n\nds[\"train\"][15]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The collate_fn batches variable-sized images by padding them to match the largest image in the batch and stacks them into a single tensor (pixel_values). It also collects the corresponding labels and optionally creates a pixel_mask to indicate which parts of the image are real (1) and which are padded (0). This ensures the model can process all inputs uniformly during training.","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef collate_fn(batch):\n    data = {}\n    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n    data[\"labels\"] = [x[\"labels\"] for x in batch]\n    if \"pixel_mask\" in batch[0]:\n        data[\"pixel_mask\"] = torch.stack([x[\"pixel_mask\"] for x in batch])\n    return data\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Intermediate format of boxes used for training is YOLO (normalized) but I will compute metrics for boxes in Pascal VOC (absolute) format in order to correctly handle box areas. Let’s define a function that converts bounding boxes to Pascal VOC format:","metadata":{}},{"cell_type":"code","source":"from transformers.image_transforms import center_to_corners_format\n\ndef convert_bbox_yolo_to_pascal(boxes, image_size):\n    \"\"\"\n    Convert bounding boxes from YOLO format (x_center, y_center, width, height) in range [0, 1]\n    to Pascal VOC format (x_min, y_min, x_max, y_max) in absolute coordinates.\n\n    Args:\n        boxes (torch.Tensor): Bounding boxes in YOLO format\n        image_size (Tuple[int, int]): Image size in format (height, width)\n\n    Returns:\n        torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\n    \"\"\"\n    # convert center to corners format\n    boxes = center_to_corners_format(boxes)\n\n    # convert to absolute coordinates\n    height, width = image_size\n    boxes = boxes * torch.tensor([[width, height, width, height]])\n\n    return boxes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"compute_metrics function we collect predicted and target bounding boxes, scores and labels from evaluation loop results and pass it to the scoring function.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom dataclasses import dataclass\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\n\n\n@dataclass\nclass ModelOutput:\n    logits: torch.Tensor\n    pred_boxes: torch.Tensor\n\n\n@torch.no_grad()\ndef compute_metrics(evaluation_results, image_processor, threshold=0.0, id2label=None):\n    \"\"\"\n    Compute mean average mAP, mAR and their variants for the object detection task.\n\n    Args:\n        evaluation_results (EvalPrediction): Predictions and targets from evaluation.\n        threshold (float, optional): Threshold to filter predicted boxes by confidence. Defaults to 0.0.\n        id2label (Optional[dict], optional): Mapping from class id to class name. Defaults to None.\n\n    Returns:\n        Mapping[str, float]: Metrics in a form of dictionary {<metric_name>: <metric_value>}\n    \"\"\"\n\n    predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n\n    # For metric computation we need to provide:\n    #  - targets in a form of list of dictionaries with keys \"boxes\", \"labels\"\n    #  - predictions in a form of list of dictionaries with keys \"boxes\", \"scores\", \"labels\"\n\n    image_sizes = []\n    post_processed_targets = []\n    post_processed_predictions = []\n\n    # Collect targets in the required format for metric computation\n    for batch in targets:\n        # collect image sizes, we will need them for predictions post processing\n        batch_image_sizes = torch.tensor(np.array([x[\"orig_size\"] for x in batch]))\n        image_sizes.append(batch_image_sizes)\n        # collect targets in the required format for metric computation\n        # boxes were converted to YOLO format needed for model training\n        # here we will convert them to Pascal VOC format (x_min, y_min, x_max, y_max)\n        for image_target in batch:\n            boxes = torch.tensor(image_target[\"boxes\"])\n            boxes = convert_bbox_yolo_to_pascal(boxes, image_target[\"orig_size\"])\n            labels = torch.tensor(image_target[\"class_labels\"])\n            post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n\n    # Collect predictions in the required format for metric computation,\n    # model produce boxes in YOLO format, then image_processor convert them to Pascal VOC format\n    for batch, target_sizes in zip(predictions, image_sizes):\n        batch_logits, batch_boxes = batch[1], batch[2]\n        output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n        post_processed_output = image_processor.post_process_object_detection(\n            output, threshold=threshold, target_sizes=target_sizes\n        )\n        post_processed_predictions.extend(post_processed_output)\n\n    # Compute metrics\n    metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n    metric.update(post_processed_predictions, post_processed_targets)\n    metrics = metric.compute()\n\n    # Replace list of per class metrics with separate metric for each class\n    classes = metrics.pop(\"classes\")\n    map_per_class = metrics.pop(\"map_per_class\")\n    mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n    for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n        class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n        metrics[f\"map_{class_name}\"] = class_map\n        metrics[f\"mar_100_{class_name}\"] = class_mar\n\n    metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n\n    return metrics\n\n\neval_compute_metrics_fn = partial(\n    compute_metrics, image_processor=image_processor, id2label=id2label, threshold=0.0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training the detection model","metadata":{}},{"cell_type":"code","source":"#import model\nfrom transformers import AutoModelForObjectDetection\n\nmodel = AutoModelForObjectDetection.from_pretrained(\n    model_name,\n    ignore_mismatched_sizes=True,\n    id2label=id2label,\n    label2id=label2id,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# setting training arguments\nfrom transformers import TrainingArguments, Trainer\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"detr_finetuned_animals\",\n    run_name=\"object-detection-animals\",\n    num_train_epochs=30,\n    fp16=False,\n    per_device_train_batch_size=8,\n    dataloader_num_workers=0,\n    learning_rate=5e-5,\n    lr_scheduler_type=\"cosine\",\n    weight_decay=1e-4,\n    max_grad_norm=0.01,\n    metric_for_best_model=\"eval_map\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    eval_do_concat_batches=False,\n    push_to_hub=False,\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds[\"train\"],\n    eval_dataset=ds[\"validation\"],\n    # processing_class=image_processor,\n    data_collator=collate_fn,\n    compute_metrics=eval_compute_metrics_fn,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"from pprint import pprint\n\nmetrics = trainer.evaluate(eval_dataset=ds[\"test\"], metric_key_prefix=\"test\")\npprint(metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"import torch\nimport requests\n\nfrom PIL import Image, ImageDraw\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\n\nurl = \"\"\nimage = Image.open(requests.get(url, stream=True).raw)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from accelerate.test_utils.testing import get_backend\ndevice, _, _ = get_backend()\nmodel_repo = \"\"\n\nimage_processor = AutoImageProcessor.from_pretrained(model_repo)\nmodel = AutoModelForObjectDetection.from_pretrained(model_repo)\nmodel = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nwith torch.no_grad():\n    inputs = image_processor(images=[image], return_tensors=\"pt\")\n    outputs = model(**inputs.to(device))\n    target_sizes = torch.tensor([[image.size[1], image.size[0]]])\n    results = image_processor.post_process_object_detection(outputs, threshold=0.3, target_sizes=target_sizes)[0]\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n        f\"{round(score.item(), 3)} at location {box}\"\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let’s plot the result:\ndraw = ImageDraw.Draw(image)\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    x, y, x2, y2 = tuple(box)\n    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n    draw.text((x, y), model.config.id2label[label.item()], fill=\"white\")\n\nimage","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}